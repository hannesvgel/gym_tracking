{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Model Test\n",
    "This notebook tests the defined models using a gradio inteface where one can simply upload an image or video, choose a model and then get a classification of the perfromed exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hannes\\01_Code\\02_master_rci\\03_Semester_PoliMi\\02_nearables_lab\\gym_tracking\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\hannes\\AppData\\Local\\Temp\\ipykernel_2672\\538120499.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  MODEL_PATH = \"..\\skeleton_lstm_multiclass3.h5\"\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "MODEL_PATH = \"..\\skeleton_lstm_multiclass3.h5\"\n",
    "# bench_press: 0, bulgarian_squat: 1, lat_machine: 2, pull_up: 3, push_up: 4, split_squat: 5\n",
    "CLASSES = [\"pull_up\", \"push_up\", \"split_squat\"]\n",
    "\n",
    "KEYPOINT_DIM = 132  # 33 landmarks with x,y,z,visibility\n",
    "\n",
    "# ——— load trained model ———\n",
    "model = tf.keras.models.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— init Mediapipe Pose & etc. ———\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— helper to extract keypoints array from Mediapipe results ———\n",
    "def extract_keypoints_from_results(results):\n",
    "    # build list in the same order as training: (lm0.x, lm0.y, lm0.z, lm0.v, lm1.x, …)\n",
    "    kpts = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        kpts.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    return np.array(kpts, dtype=np.float32)  # shape = (132,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLIPPED\n",
    "def process_video_flipped(video):\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    sequence_flipped = []  # Only store flipped keypoints\n",
    "    frame_count = 0\n",
    "    max_frames = 30\n",
    "    \n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert frame to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get pose landmarks\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extract keypoints\n",
    "            keypoints = extract_keypoints_from_results(results)\n",
    "            \n",
    "            # Create flipped keypoints by negating x coordinates\n",
    "            keypoints_flipped = keypoints.copy()\n",
    "            for i in range(0, len(keypoints_flipped), 4):  # Step by 4 because each point has x,y,z,visibility\n",
    "                keypoints_flipped[i] = 1-keypoints_flipped[i]  # Flip x coordinate\n",
    "            \n",
    "            sequence_flipped.append(keypoints_flipped)\n",
    "            frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Pad sequence if needed\n",
    "    if len(sequence_flipped) < max_frames:\n",
    "        last_frame = sequence_flipped[-1] if sequence_flipped else np.zeros(KEYPOINT_DIM)\n",
    "        while len(sequence_flipped) < max_frames:\n",
    "            sequence_flipped.append(last_frame)\n",
    "    \n",
    "    # Convert sequence to numpy array and reshape for model input\n",
    "    sequence_flipped = np.array(sequence_flipped).reshape(1, max_frames, KEYPOINT_DIM)\n",
    "    \n",
    "    # Get model predictions\n",
    "    predictions = model.predict(sequence_flipped, verbose=0)[0]\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top_3_idx = np.argsort(predictions)[-3:][::-1]\n",
    "    top_3_classes = [CLASSES[i] for i in top_3_idx]\n",
    "    top_3_confidences = [float(predictions[i]) for i in top_3_idx]\n",
    "    \n",
    "    # Create prediction text\n",
    "    prediction_text = \"Top 3 Predictions (Flipped):\\n\"\n",
    "    for i in range(3):\n",
    "        prediction_text += f\"{i+1}. {top_3_classes[i]}: {top_3_confidences[i]:.2%}\\n\"\n",
    "    \n",
    "    # Create a visualization of the flipped pose\n",
    "    if len(sequence_flipped) > 0:\n",
    "        # Create a blank image\n",
    "        last_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Create a new NormalizedLandmarkList for the flipped pose\n",
    "        flipped_landmarks = landmark_pb2.NormalizedLandmarkList()\n",
    "        \n",
    "        # Get the last keypoints from the sequence\n",
    "        last_keypoints = sequence_flipped[0, -1]  # Shape: (132,)\n",
    "        \n",
    "        # Convert keypoints back to landmark format\n",
    "        for i in range(0, len(last_keypoints), 4):\n",
    "            landmark = flipped_landmarks.landmark.add()\n",
    "            landmark.x = last_keypoints[i]\n",
    "            landmark.y = last_keypoints[i + 1]\n",
    "            landmark.z = last_keypoints[i + 2]\n",
    "            landmark.visibility = last_keypoints[i + 3]\n",
    "        \n",
    "        # Draw flipped pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            last_frame,\n",
    "            flipped_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS\n",
    "        )\n",
    "        \n",
    "        # Add prediction text to frame\n",
    "        y_position = 30\n",
    "        for i in range(3):\n",
    "            text = f\"{top_3_classes[i]}: {top_3_confidences[i]:.2%}\"\n",
    "            cv2.putText(last_frame, text, (10, y_position), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            y_position += 40\n",
    "    else:\n",
    "        last_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        cv2.putText(last_frame, \"No pose detected in video\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    return prediction_text, last_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gradio interface for flipped version\n",
    "demo_flipped = gr.Interface(\n",
    "    fn=process_video_flipped,\n",
    "    inputs=gr.Video(),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Predictions (Flipped)\", lines=4),\n",
    "        gr.Image(label=\"Flipped Pose Visualization\")\n",
    "    ],\n",
    "    title=\"Exercise Classification (Flipped)\",\n",
    "    description=\"Upload a video to classify the exercise using flipped pose keypoints.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo_flipped.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "def process_video(video):\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    sequence = []\n",
    "    frame_count = 0\n",
    "    max_frames = 30  # We want 30 frames for the sequence\n",
    "    \n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert frame to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get pose landmarks\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extract keypoints\n",
    "            keypoints = extract_keypoints_from_results(results)\n",
    "            sequence.append(keypoints)\n",
    "            frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # If we don't have enough frames, pad the sequence\n",
    "    if len(sequence) < max_frames:\n",
    "        # Pad with the last frame's keypoints\n",
    "        last_frame = sequence[-1] if sequence else np.zeros(KEYPOINT_DIM)\n",
    "        while len(sequence) < max_frames:\n",
    "            sequence.append(last_frame)\n",
    "    \n",
    "    # Convert sequence to numpy array and reshape for model input\n",
    "    sequence = np.array(sequence)\n",
    "    sequence = sequence.reshape(1, max_frames, KEYPOINT_DIM)\n",
    "    \n",
    "    # Get model predictions\n",
    "    predictions = model.predict(sequence, verbose=0)[0]\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top_3_idx = np.argsort(predictions)[-3:][::-1]\n",
    "    top_3_classes = [CLASSES[i] for i in top_3_idx]\n",
    "    top_3_confidences = [float(predictions[i]) for i in top_3_idx]\n",
    "    \n",
    "    # Create prediction text\n",
    "    prediction_text = \"Top 3 Predictions:\\n\"\n",
    "    for i in range(3):\n",
    "        prediction_text += f\"{i+1}. {top_3_classes[i]}: {top_3_confidences[i]:.2%}\\n\"\n",
    "    \n",
    "    # Create a visualization of the last processed frame\n",
    "    if len(sequence) > 0:\n",
    "        last_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            last_frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "        # Add prediction text to frame\n",
    "        y_position = 30\n",
    "        for i in range(3):\n",
    "            text = f\"{top_3_classes[i]}: {top_3_confidences[i]:.2%}\"\n",
    "            cv2.putText(last_frame, text, (10, y_position), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            y_position += 40\n",
    "    else:\n",
    "        last_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        cv2.putText(last_frame, \"No pose detected in video\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    return prediction_text, last_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=process_video,\n",
    "    inputs=gr.Video(),  # Changed from Image to Video\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Predictions\", lines=4),\n",
    "        gr.Image(label=\"Last Frame with Pose\")\n",
    "    ],\n",
    "    title=\"Exercise Classification (Video)\",\n",
    "    description=\"Upload a video to classify the exercise being performed.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results 16.06.2025\n",
    "\n",
    "### Findings \n",
    "- **Point-of-View / Flipping Effect**: <br> when flipping the keypoints horizontally (i.e. mirroring left <-> right), the model changes its prediction for the same original video. Strongly suggests the model is “looking” at absolute x-positions in the frame (e.g. left-side bias), rather than the relative geometry of the joints.\n",
    "- **Overfitting** <br> model performance drops on flipped (but otherwise identical) inputs, it’s likely memorizing frame-specific patterns (camera offset, background context) rather than learning the invariant shape of the exercise. \n",
    "- **Class imbalance** <br> between the processed csv files; low-sample classes vulnerable to misclassification:\n",
    "    - bench_press: 387 elements\n",
    "    - bulgarian_squat: 452 elements\n",
    "    - lat_machine: 333 elements\n",
    "    - pull_up: 167 elements\n",
    "    - push_up: 231 elements\n",
    "    - split_squat: 665 elements\n",
    "- **Misclassification Bias** <br> a lot of times for 3 class classification push up or pull up are misclassified to split_squat -> bias towards overrepresented class; common for under-represented classes to be “sucked into” an overrepresented neighbor\n",
    "\n",
    "### Mitigation Steps\n",
    "- **Data Augmentation**:\n",
    "    - flip/rotate all the input videos by a certain margin\n",
    "    - add them twice: once normal & once vertical flipped (remember to swap left/right landmark indices (e.g. left_elbow <-> right_elbow) before training)\n",
    "- **Expand Dataset**:\n",
    "    - More videos from different angles, lighting, and subjects will improve generalization.\n",
    "    - one may have to abandon and replace given classes such as split_squat & bulgarian_squat as they do not exist in big public datasets\n",
    "- **Mediapipe** <br> normalized coordinated at top left af image, all coordinates are between [0, 1]. one could consider some improvements:\n",
    "    - **Center on a root joint** <br> Subtract the hip midpoint (or another stable landmark) from every x,y,z so that your model sees poses relative to the body, not the image\n",
    "    - **Scale Normalization** <br> Divide distances by the length of the torso or shoulders width so that “big” vs. “small” people don’t confuse the network\n",
    "    - **Hand-crafted Features** <br> calculate distances and angles and add as extra input to the model to give it more information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
