{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Model Test\n",
    "This notebook tests the defined models using a gradio inteface where one can simply upload an image or video, choose a model and then get a classification of the perfromed exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\hannes\\AppData\\Local\\Temp\\ipykernel_15036\\2996044664.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  MODEL_PATH = \"..\\skeleton_lstm_bidir_multiclass6_v3.h5\"\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "MODEL_PATH = \"..\\skeleton_lstm_bidir_multiclass6_v3.h5\"\n",
    "# bench_press: 0, lat_machine: 1, pull_up: 2, push_up: 3, squat: 4, split_squat: 5\n",
    "CLASSES      = [\"bench_press\" ,\"lat_machine\", \"pull_up\", \"push_up\", \"squat\", \"split_squat\"]\n",
    "\n",
    "KEYPOINT_DIM = 132  # 33 landmarks with x,y,z,visibility\n",
    "\n",
    "# ——— load trained model ———\n",
    "model = tf.keras.models.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— init Mediapipe Pose & etc. ———\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— helper to extract keypoints array from Mediapipe results ———\n",
    "def extract_keypoints_from_results(results):\n",
    "    # build list in the same order as training: (lm0.x, lm0.y, lm0.z, lm0.v, lm1.x, …)\n",
    "    kpts = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        kpts.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    return np.array(kpts, dtype=np.float32)  # shape = (132,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL\n",
    "def process_video(video):\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    # Get total number of frames\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Calculate start frame for middle 30 frames\n",
    "    max_frames = 30\n",
    "    if total_frames <= max_frames:\n",
    "        start_frame = 0\n",
    "    else:\n",
    "        start_frame = (total_frames - max_frames) // 2\n",
    "    \n",
    "    # Set the starting position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "    sequence = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert frame to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get pose landmarks\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extract keypoints\n",
    "            keypoints = extract_keypoints_from_results(results)\n",
    "            sequence.append(keypoints)\n",
    "            frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # If we don't have enough frames, pad the sequence\n",
    "    if len(sequence) < max_frames:\n",
    "        # Pad with the last frame's keypoints\n",
    "        last_frame = sequence[-1] if sequence else np.zeros(KEYPOINT_DIM)\n",
    "        while len(sequence) < max_frames:\n",
    "            sequence.append(last_frame)\n",
    "    \n",
    "    # Convert sequence to numpy array and reshape for model input\n",
    "    sequence = np.array(sequence)\n",
    "    sequence = sequence.reshape(1, max_frames, KEYPOINT_DIM)\n",
    "    \n",
    "    # Get model predictions\n",
    "    predictions = model.predict(sequence, verbose=0)[0]\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top_3_idx = np.argsort(predictions)[-3:][::-1]\n",
    "    top_3_classes = [CLASSES[i] for i in top_3_idx]\n",
    "    top_3_confidences = [float(predictions[i]) for i in top_3_idx]\n",
    "    \n",
    "    # Create prediction text\n",
    "    prediction_text = \"Top 3 Predictions:\\n\"\n",
    "    for i in range(3):\n",
    "        prediction_text += f\"{i+1}. {top_3_classes[i]}: {top_3_confidences[i]:.2%}\\n\"\n",
    "    \n",
    "    # Create a visualization of the last processed frame\n",
    "    if len(sequence) > 0:\n",
    "        last_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            last_frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "        # Add prediction text to frame\n",
    "        y_position = 30\n",
    "        for i in range(3):\n",
    "            text = f\"{top_3_classes[i]}: {top_3_confidences[i]:.2%}\"\n",
    "            cv2.putText(last_frame, text, (10, y_position), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            y_position += 40\n",
    "    else:\n",
    "        last_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        cv2.putText(last_frame, \"No pose detected in video\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    return prediction_text, last_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=process_video,\n",
    "    inputs=gr.Video(),  # Changed from Image to Video\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Predictions\", lines=4),\n",
    "        gr.Image(label=\"Last Frame with Pose\")\n",
    "    ],\n",
    "    title=\"Exercise Classification (Video)\",\n",
    "    description=\"Upload a video to classify the exercise being performed.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WHOLE VIDEO SEGMENTED CLASSIFICATION\n",
    "\n",
    "def process_video_segments(video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    max_frames = 30\n",
    "    sequence = []\n",
    "    segment_predictions = []\n",
    "    class_counts = {cls: 0 for cls in CLASSES}\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(frame_rgb)\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = extract_keypoints_from_results(results)\n",
    "            sequence.append(keypoints)\n",
    "        else:\n",
    "            # If no pose, pad with zeros\n",
    "            sequence.append(np.zeros(KEYPOINT_DIM))\n",
    "        \n",
    "        # When we have 30 frames, make a prediction\n",
    "        if len(sequence) == max_frames:\n",
    "            segment_np = np.array(sequence).reshape(1, max_frames, KEYPOINT_DIM)\n",
    "            predictions = model.predict(segment_np, verbose=0)[0]\n",
    "            pred_idx = np.argmax(predictions)\n",
    "            pred_class = CLASSES[pred_idx]\n",
    "            segment_predictions.append(pred_class)\n",
    "            class_counts[pred_class] += 1\n",
    "            sequence = []  # Start next segment\n",
    "    cap.release()\n",
    "    \n",
    "    # If there are leftover frames, pad and predict\n",
    "    if 0 < len(sequence) < max_frames:\n",
    "        last_frame = sequence[-1] if sequence else np.zeros(KEYPOINT_DIM)\n",
    "        while len(sequence) < max_frames:\n",
    "            sequence.append(last_frame)\n",
    "        segment_np = np.array(sequence).reshape(1, max_frames, KEYPOINT_DIM)\n",
    "        predictions = model.predict(segment_np, verbose=0)[0]\n",
    "        pred_idx = np.argmax(predictions)\n",
    "        pred_class = CLASSES[pred_idx]\n",
    "        segment_predictions.append(pred_class)\n",
    "        class_counts[pred_class] += 1\n",
    "    \n",
    "    # Find the most frequent class\n",
    "    if segment_predictions:\n",
    "        from collections import Counter\n",
    "        most_common_class, most_common_count = Counter(segment_predictions).most_common(1)[0]\n",
    "    else:\n",
    "        most_common_class, most_common_count = None, 0\n",
    "    \n",
    "    # Prepare output\n",
    "    result_text = f\"Most frequent class: {most_common_class} ({most_common_count} segments)\\n\"\n",
    "    result_text += \"\\nClass counts:\\n\"\n",
    "    for cls, count in class_counts.items():\n",
    "        result_text += f\"{cls}: {count}\\n\"\n",
    "    \n",
    "    return result_text\n",
    "\n",
    "# Example Gradio interface for whole video classification\n",
    "demo_segments = gr.Interface(\n",
    "    fn=process_video_segments,\n",
    "    inputs=gr.Video(),\n",
    "    outputs=gr.Textbox(label=\"Whole Video Segment Classification\", lines=8),\n",
    "    title=\"Exercise Classification (Whole Video Segments)\",\n",
    "    description=\"Upload a video to classify the exercise in 30-frame segments. Shows the most frequent class and counts for all classes observed.\"\n",
    ")\n",
    "\n",
    "demo_segments.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results 16.06.2025\n",
    "\n",
    "### Findings \n",
    "- **Point-of-View / Flipping Effect** <br> when flipping the keypoints horizontally (i.e. mirroring left <-> right), the model changes its prediction for the same original video. Strongly suggests the model is “looking” at absolute x-positions in the frame (e.g. left-side bias), rather than the relative geometry of the joints.\n",
    "- **Overfitting** <br> model performance drops on flipped (but otherwise identical) inputs, it’s likely memorizing frame-specific patterns (camera offset, background context) rather than learning the invariant shape of the exercise. \n",
    "- **Class imbalance** <br> between the processed csv files; low-sample classes vulnerable to misclassification:\n",
    "    - bench_press: 387 elements\n",
    "    - bulgarian_squat: 452 elements\n",
    "    - lat_machine: 333 elements\n",
    "    - pull_up: 167 elements\n",
    "    - push_up: 231 elements\n",
    "    - split_squat: 665 elements\n",
    "- **Misclassification Bias** <br> a lot of times for 3 class classification push up or pull up are misclassified to split_squat -> bias towards overrepresented class; common for under-represented classes to be “sucked into” an overrepresented neighbor\n",
    "\n",
    "### Mitigation Steps\n",
    "- **Data Augmentation**\n",
    "    - flip/rotate all the input videos by a certain margin\n",
    "    - add them twice: once normal & once vertical flipped (remember to swap left/right landmark indices (e.g. left_elbow <-> right_elbow) before training)\n",
    "    - change framesize (e.g. 50 frames)\n",
    "- **Expand Dataset**\n",
    "    - More videos from different angles, lighting, and subjects will improve generalization.\n",
    "    - one may have to abandon and replace given classes such as split_squat & bulgarian_squat as they do not exist in big public datasets\n",
    "- **Mediapipe** <br> normalized coordinated at top left af image, all coordinates are between [0, 1]. one could consider some improvements:\n",
    "    - **Center on a root joint** <br> Subtract the hip midpoint (or another stable landmark) from every x,y,z so that your model sees poses relative to the body, not the image\n",
    "    - **Scale Normalization** <br> Divide distances by the length of the torso or shoulders width so that “big” vs. “small” people don’t confuse the network\n",
    "    - **Hand-crafted Features** <br> calculate distances and angles and add as extra input to the model to give it more information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
