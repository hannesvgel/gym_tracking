{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Model Test\n",
    "This notebook tests the defined models using a gradio inteface where one can simply upload an image or video, choose a model and then get a classification of the perfromed exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\hannes\\AppData\\Local\\Temp\\ipykernel_10264\\538120499.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  MODEL_PATH = \"..\\skeleton_lstm_multiclass3.h5\"\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "MODEL_PATH = \"..\\skeleton_lstm_multiclass3.h5\"\n",
    "# bench_press: 0, bulgarian_squat: 1, lat_machine: 2, pull_up: 3, push_up: 4, split_squat: 5\n",
    "CLASSES = [\"pull_up\", \"push_up\", \"split_squat\"]\n",
    "\n",
    "KEYPOINT_DIM = 132  # 33 landmarks with x,y,z,visibility\n",
    "\n",
    "# ——— load trained model ———\n",
    "model = tf.keras.models.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— init Mediapipe Pose & etc. ———\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— helper to extract keypoints array from Mediapipe results ———\n",
    "def extract_keypoints_from_results(results):\n",
    "    # build list in the same order as training: (lm0.x, lm0.y, lm0.z, lm0.v, lm1.x, …)\n",
    "    kpts = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        kpts.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "    return np.array(kpts, dtype=np.float32)  # shape = (132,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def process_video(video):\\n    # Initialize video capture\\n    cap = cv2.VideoCapture(video)\\n    sequence = []\\n    frame_count = 0\\n    max_frames = 30  # We want 30 frames for the sequence\\n\\n    while cap.isOpened() and frame_count < max_frames:\\n        ret, frame = cap.read()\\n        if not ret:\\n            break\\n\\n        # Convert frame to RGB for MediaPipe\\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n\\n        # Get pose landmarks\\n        results = pose.process(frame_rgb)\\n\\n        if results.pose_landmarks:\\n            # Extract keypoints\\n            keypoints = extract_keypoints_from_results(results)\\n            sequence.append(keypoints)\\n            frame_count += 1\\n\\n    cap.release()\\n\\n    # If we don\\'t have enough frames, pad the sequence\\n    if len(sequence) < max_frames:\\n        # Pad with the last frame\\'s keypoints\\n        last_frame = sequence[-1] if sequence else np.zeros(KEYPOINT_DIM)\\n        while len(sequence) < max_frames:\\n            sequence.append(last_frame)\\n\\n    # Convert sequence to numpy array and reshape for model input\\n    sequence = np.array(sequence)\\n    sequence = sequence.reshape(1, max_frames, KEYPOINT_DIM)\\n\\n    # Get model predictions\\n    predictions = model.predict(sequence, verbose=0)[0]\\n\\n    # Get top 3 predictions\\n    top_3_idx = np.argsort(predictions)[-3:][::-1]\\n    top_3_classes = [CLASSES[i] for i in top_3_idx]\\n    top_3_confidences = [float(predictions[i]) for i in top_3_idx]\\n\\n    # Create prediction text\\n    prediction_text = \"Top 3 Predictions:\\n\"\\n    for i in range(3):\\n        prediction_text += f\"{i+1}. {top_3_classes[i]}: {top_3_confidences[i]:.2%}\\n\"\\n\\n    # Create a visualization of the last processed frame\\n    if len(sequence) > 0:\\n        last_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n        # Draw pose landmarks\\n        mp_drawing.draw_landmarks(\\n            last_frame,\\n            results.pose_landmarks,\\n            mp_pose.POSE_CONNECTIONS,\\n            landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style()\\n        )\\n\\n        # Add prediction text to frame\\n        y_position = 30\\n        for i in range(3):\\n            text = f\"{top_3_classes[i]}: {top_3_confidences[i]:.2%}\"\\n            cv2.putText(last_frame, text, (10, y_position), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\\n            y_position += 40\\n    else:\\n        last_frame = np.zeros((480, 640, 3), dtype=np.uint8)\\n        cv2.putText(last_frame, \"No pose detected in video\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\\n\\n    return prediction_text, last_frame'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ORIGINAL\n",
    "'''def process_video(video):\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    sequence = []\n",
    "    frame_count = 0\n",
    "    max_frames = 30  # We want 30 frames for the sequence\n",
    "    \n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert frame to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get pose landmarks\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extract keypoints\n",
    "            keypoints = extract_keypoints_from_results(results)\n",
    "            sequence.append(keypoints)\n",
    "            frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # If we don't have enough frames, pad the sequence\n",
    "    if len(sequence) < max_frames:\n",
    "        # Pad with the last frame's keypoints\n",
    "        last_frame = sequence[-1] if sequence else np.zeros(KEYPOINT_DIM)\n",
    "        while len(sequence) < max_frames:\n",
    "            sequence.append(last_frame)\n",
    "    \n",
    "    # Convert sequence to numpy array and reshape for model input\n",
    "    sequence = np.array(sequence)\n",
    "    sequence = sequence.reshape(1, max_frames, KEYPOINT_DIM)\n",
    "    \n",
    "    # Get model predictions\n",
    "    predictions = model.predict(sequence, verbose=0)[0]\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top_3_idx = np.argsort(predictions)[-3:][::-1]\n",
    "    top_3_classes = [CLASSES[i] for i in top_3_idx]\n",
    "    top_3_confidences = [float(predictions[i]) for i in top_3_idx]\n",
    "    \n",
    "    # Create prediction text\n",
    "    prediction_text = \"Top 3 Predictions:\\n\"\n",
    "    for i in range(3):\n",
    "        prediction_text += f\"{i+1}. {top_3_classes[i]}: {top_3_confidences[i]:.2%}\\n\"\n",
    "    \n",
    "    # Create a visualization of the last processed frame\n",
    "    if len(sequence) > 0:\n",
    "        last_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            last_frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "        # Add prediction text to frame\n",
    "        y_position = 30\n",
    "        for i in range(3):\n",
    "            text = f\"{top_3_classes[i]}: {top_3_confidences[i]:.2%}\"\n",
    "            cv2.putText(last_frame, text, (10, y_position), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            y_position += 40\n",
    "    else:\n",
    "        last_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        cv2.putText(last_frame, \"No pose detected in video\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    return prediction_text, last_frame'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FLIPPED\n",
    "def process_video_flipped(video):\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    sequence_flipped = []  # Only store flipped keypoints\n",
    "    frame_count = 0\n",
    "    max_frames = 30\n",
    "    \n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert frame to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get pose landmarks\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extract keypoints\n",
    "            keypoints = extract_keypoints_from_results(results)\n",
    "            \n",
    "            # Create flipped keypoints by negating x coordinates\n",
    "            keypoints_flipped = keypoints.copy()\n",
    "            for i in range(0, len(keypoints_flipped), 4):  # Step by 4 because each point has x,y,z,visibility\n",
    "                keypoints_flipped[i] = 1-keypoints_flipped[i]  # Flip x coordinate\n",
    "            \n",
    "            sequence_flipped.append(keypoints_flipped)\n",
    "            frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Pad sequence if needed\n",
    "    if len(sequence_flipped) < max_frames:\n",
    "        last_frame = sequence_flipped[-1] if sequence_flipped else np.zeros(KEYPOINT_DIM)\n",
    "        while len(sequence_flipped) < max_frames:\n",
    "            sequence_flipped.append(last_frame)\n",
    "    \n",
    "    # Convert sequence to numpy array and reshape for model input\n",
    "    sequence_flipped = np.array(sequence_flipped).reshape(1, max_frames, KEYPOINT_DIM)\n",
    "    \n",
    "    # Get model predictions\n",
    "    predictions = model.predict(sequence_flipped, verbose=0)[0]\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top_3_idx = np.argsort(predictions)[-3:][::-1]\n",
    "    top_3_classes = [CLASSES[i] for i in top_3_idx]\n",
    "    top_3_confidences = [float(predictions[i]) for i in top_3_idx]\n",
    "    \n",
    "    # Create prediction text\n",
    "    prediction_text = \"Top 3 Predictions (Flipped):\\n\"\n",
    "    for i in range(3):\n",
    "        prediction_text += f\"{i+1}. {top_3_classes[i]}: {top_3_confidences[i]:.2%}\\n\"\n",
    "    \n",
    "    # Create a visualization of the flipped pose\n",
    "    if len(sequence_flipped) > 0:\n",
    "        # Create a blank image\n",
    "        last_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Create a new NormalizedLandmarkList for the flipped pose\n",
    "        flipped_landmarks = landmark_pb2.NormalizedLandmarkList()\n",
    "        \n",
    "        # Get the last keypoints from the sequence\n",
    "        last_keypoints = sequence_flipped[0, -1]  # Shape: (132,)\n",
    "        \n",
    "        # Convert keypoints back to landmark format\n",
    "        for i in range(0, len(last_keypoints), 4):\n",
    "            landmark = flipped_landmarks.landmark.add()\n",
    "            landmark.x = last_keypoints[i]\n",
    "            landmark.y = last_keypoints[i + 1]\n",
    "            landmark.z = last_keypoints[i + 2]\n",
    "            landmark.visibility = last_keypoints[i + 3]\n",
    "        \n",
    "        # Draw flipped pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            last_frame,\n",
    "            flipped_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS\n",
    "        )\n",
    "        \n",
    "        # Add prediction text to frame\n",
    "        y_position = 30\n",
    "        for i in range(3):\n",
    "            text = f\"{top_3_classes[i]}: {top_3_confidences[i]:.2%}\"\n",
    "            cv2.putText(last_frame, text, (10, y_position), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            y_position += 40\n",
    "    else:\n",
    "        last_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        cv2.putText(last_frame, \"No pose detected in video\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    return prediction_text, last_frame\n",
    "\n",
    "# Create Gradio interface for flipped version\n",
    "demo_flipped = gr.Interface(\n",
    "    fn=process_video_flipped,\n",
    "    inputs=gr.Video(),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Predictions (Flipped)\", lines=4),\n",
    "        gr.Image(label=\"Flipped Pose Visualization\")\n",
    "    ],\n",
    "    title=\"Exercise Classification (Flipped)\",\n",
    "    description=\"Upload a video to classify the exercise using flipped pose keypoints.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo_flipped.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLIPPED + ORIGINAL\n",
    "def process_video(video):\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    sequence = []\n",
    "    sequence_flipped = []  # New sequence for flipped keypoints\n",
    "    frame_count = 0\n",
    "    max_frames = 30\n",
    "    \n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert frame to RGB for MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get pose landmarks\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extract keypoints\n",
    "            keypoints = extract_keypoints_from_results(results)\n",
    "            \n",
    "            # Create flipped keypoints by negating x coordinates\n",
    "            keypoints_flipped = keypoints.copy()\n",
    "            for i in range(0, len(keypoints_flipped), 4):  # Step by 4 because each point has x,y,z,visibility\n",
    "                keypoints_flipped[i] = -keypoints_flipped[i]  # Flip x coordinate\n",
    "            \n",
    "            sequence.append(keypoints)\n",
    "            sequence_flipped.append(keypoints_flipped)\n",
    "            frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Pad sequences if needed\n",
    "    if len(sequence) < max_frames:\n",
    "        last_frame = sequence[-1] if sequence else np.zeros(KEYPOINT_DIM)\n",
    "        last_frame_flipped = sequence_flipped[-1] if sequence_flipped else np.zeros(KEYPOINT_DIM)\n",
    "        while len(sequence) < max_frames:\n",
    "            sequence.append(last_frame)\n",
    "            sequence_flipped.append(last_frame_flipped)\n",
    "    \n",
    "    # Convert sequences to numpy arrays and reshape for model input\n",
    "    sequence = np.array(sequence).reshape(1, max_frames, KEYPOINT_DIM)\n",
    "    sequence_flipped = np.array(sequence_flipped).reshape(1, max_frames, KEYPOINT_DIM)\n",
    "    \n",
    "    # Get model predictions for both sequences\n",
    "    predictions = model.predict(sequence, verbose=0)[0]\n",
    "    predictions_flipped = model.predict(sequence_flipped, verbose=0)[0]\n",
    "    \n",
    "    # Combine predictions (take maximum confidence for each class)\n",
    "    combined_predictions = np.maximum(predictions, predictions_flipped)\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top_3_idx = np.argsort(combined_predictions)[-3:][::-1]\n",
    "    top_3_classes = [CLASSES[i] for i in top_3_idx]\n",
    "    top_3_confidences = [float(combined_predictions[i]) for i in top_3_idx]\n",
    "    \n",
    "    # Create prediction text\n",
    "    prediction_text = \"Top 3 Predictions:\\n\"\n",
    "    for i in range(3):\n",
    "        prediction_text += f\"{i+1}. {top_3_classes[i]}: {top_3_confidences[i]:.2%}\\n\"\n",
    "    \n",
    "    # Create a visualization of the last processed frame\n",
    "    if len(sequence) > 0:\n",
    "        last_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Draw pose landmarks\n",
    "        mp_drawing.draw_landmarks(\n",
    "            last_frame,\n",
    "            results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "        \n",
    "        # Add prediction text to frame\n",
    "        y_position = 30\n",
    "        for i in range(3):\n",
    "            text = f\"{top_3_classes[i]}: {top_3_confidences[i]:.2%}\"\n",
    "            cv2.putText(last_frame, text, (10, y_position), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            y_position += 40\n",
    "    else:\n",
    "        last_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        cv2.putText(last_frame, \"No pose detected in video\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    return prediction_text, last_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7885\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7885/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=process_video,\n",
    "    inputs=gr.Video(),  # Changed from Image to Video\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Predictions\", lines=4),\n",
    "        gr.Image(label=\"Last Frame with Pose\")\n",
    "    ],\n",
    "    title=\"Exercise Classification (Video)\",\n",
    "    description=\"Upload a video to classify the exercise being performed.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
