{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Model Test\n",
    "This notebook tests the defined models using a gradio inteface where one can simply upload an image or video, choose a model and then get a classification of the perfromed exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\hannes\\AppData\\Local\\Temp\\ipykernel_18272\\2927921311.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  MODEL_PATH = \"..\\skeleton_cnn_multiclass3.h5\"\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "MODEL_PATH = \"..\\skeleton_cnn_multiclass3.h5\"\n",
    "# bench_press: 0, bulgarian_squat: 1, lat_machine: 2, pull_up: 3, push_up: 4, split_squat: 5\n",
    "CLASSES = [\"pull_up\", \"push_up\", \"split_squat\"]\n",
    "\n",
    "KEYPOINT_DIM = 132  # 33 landmarks with x,y,z,visibility\n",
    "\n",
    "# ——— load trained model ———\n",
    "model = tf.keras.models.load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— init Mediapipe Pose & etc. ———\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ——— helper to extract keypoints array from Mediapipe results ———\n",
    "def extract_keypoints_from_results(results):\n",
    "    kpts = []\n",
    "    for lm in results.pose_landmarks.landmark:\n",
    "        kpts += [lm.x, lm.y, lm.z, lm.visibility]\n",
    "    arr = np.array(kpts, dtype=np.float32)\n",
    "    return arr.reshape(1, KEYPOINT_DIM, 1)  # shape = (1,132,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    # Convert image to RGB (MediaPipe requires RGB)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get pose landmarks\n",
    "    results = pose.process(image_rgb)\n",
    "    \n",
    "    if not results.pose_landmarks:\n",
    "        return \"No pose detected in the image\", image\n",
    "    \n",
    "    # Extract keypoints\n",
    "    keypoints = extract_keypoints_from_results(results)\n",
    "    \n",
    "    # Get model prediction\n",
    "    predictions = model.predict(keypoints, verbose=0)[0]\n",
    "    predicted_class = CLASSES[np.argmax(predictions)]\n",
    "    confidence = float(np.max(predictions))\n",
    "    \n",
    "    # Draw pose landmarks on image\n",
    "    annotated_image = image.copy()\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style()\n",
    "    )\n",
    "    \n",
    "    # Add prediction text to image\n",
    "    text = f\"{predicted_class}: {confidence:.2%}\"\n",
    "    cv2.putText(annotated_image, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    return f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2%}\", annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=process_image,\n",
    "    inputs=gr.Image(type=\"numpy\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Prediction\"),\n",
    "        gr.Image(label=\"Annotated Image\")\n",
    "    ],\n",
    "    title=\"Exercise Classification\",\n",
    "    description=\"Upload an image to classify the exercise being performed.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
